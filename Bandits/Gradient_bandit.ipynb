{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoAUaEooH9ceHZfqJMekEQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Multiarmed Bandits: Gradient Bandit algorithm**\n","\n","In this notebook, we use OpenAI Gym to implement the Gradient Bandit algorithm"],"metadata":{"id":"76tfA-bMAk9i"}},{"cell_type":"code","source":["import gym"],"metadata":{"id":"l79GNCq0HjaP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunatley, Gym does not provide a bandit environment so we need to import it, lets install one with the command below:"],"metadata":{"id":"GGl2HVXbHYzn"}},{"cell_type":"code","source":["!git clone https://github.com/JKCooper2/gym-bandits.git > /dev/null 2>&1\n","!pip install /content/gym-bandits/. > /dev/null 2>&1"],"metadata":{"id":"LDeviPzjHfa4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's import needed packages"],"metadata":{"id":"CRyZ2QEeJB3k"}},{"cell_type":"code","source":["import gym_bandits\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tqdm import tqdm"],"metadata":{"id":"1UXzA8QyH9nI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the softmax function"],"metadata":{"id":"U54G4uouJN7S"}},{"cell_type":"code","source":["def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum()"],"metadata":{"id":"9-b0XhlrJVuq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define the action following the distribution given by the softmax funciton"],"metadata":{"id":"pGE4vvwzKQ3z"}},{"cell_type":"code","source":["def get_action(h):\n","    probs = softmax(h)\n","    return np.random.choice(10, p=probs), probs"],"metadata":{"id":"xuCY0_jfv8vl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define a function that compute the average of obtained reward over repeated experiments."],"metadata":{"id":"FvYyOYJ6wD7E"}},{"cell_type":"code","source":["def average(tot_rewards):\n","    avg = []\n","    for i in range(len(tot_rewards[0])):\n","        temp = 0\n","        for j in range(len(tot_rewards)):\n","            temp += tot_rewards[j][i]\n","        temp /= len(tot_rewards)\n","        avg.append(temp)\n","    return avg"],"metadata":{"id":"UFe6XPQWKxv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Main code: define an Environment for 10-armed Bandit whose rewards are normal distributed with standard deviation = 1 and a mean drawn from a normal distribution (0,1). Same as Sutton&Barto book"],"metadata":{"id":"gJXeronaLBgR"}},{"cell_type":"code","source":["k = 10  # Number of actions (arms)\n","iter = 1000 # Number of steps\n","repeat = 2000 # Number of repetitions\n","o0 = []  # No baseline, a = 0.4\n","o00 = []  # No baseline, a = 0.1\n","e01 = [] # Baseline, a = 0.4\n","e01 = [] # Baseline, a = 0.1\n","\n","\n","\n","np.random.seed(42)\n","env = gym.make(\"BanditTenArmedGaussian-v0\") # define Environment"],"metadata":{"id":"SGTPIh92LwOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the Gradient Bandit algorith with alpha = 0.4 and No-Baseline several times to average the results"],"metadata":{"id":"dAEDMZfZL0Iq"}},{"cell_type":"code","source":["alpha = 0.4\n","\n","env.reset()\n","tot_rewards = []\n","for _ in tqdm(range(repeat)):\n","    H = np.zeros(shape=(k))\n","    rewards = []\n","    for i in range(iter):\n","        action, probs = get_action(H)\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","        H[action] = H[action] + alpha * reward * (1 - probs[action])\n","        H[:action] = H[:action]  - alpha * reward * probs[:action]\n","        if action + 1 < k:\n","            H[action + 1:] = H[action + 1:]  - alpha * reward * probs[action + 1]\n","    env.reset()\n","    tot_rewards.append(rewards)\n","\n","o0 = average(tot_rewards)"],"metadata":{"id":"UqkyUekIL9zY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the Gradient Bandit algorith with alpha = 0.4 and Baseline several times to average the results"],"metadata":{"id":"laVkkOltM2MG"}},{"cell_type":"code","source":["env.reset()\n","tot_rewards = []\n","for _ in tqdm(range(repeat)):\n","    H = np.zeros(shape=(k))\n","    R = 0\n","    rewards = []\n","    for i in range(iter):\n","        action, probs = get_action(H)\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","        H[action] = H[action] + alpha * (reward - R) * (1 - probs[action])\n","        H[:action] = H[:action]  - alpha * (reward - R) * probs[:action]\n","        if action + 1 < k:\n","            H[action + 1:] = H[action + 1:]  - alpha * (reward - R) * probs[action + 1]\n","        R = R + 1/(i + 1) * (reward - R)\n","    env.reset()\n","    tot_rewards.append(rewards)\n","\n","e01 = average(tot_rewards)"],"metadata":{"id":"Zlp6V7tPM-5E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the Gradient Bandit algorith with alpha = 0.1 and No-Baseline several times to average the results"],"metadata":{"id":"6JYtgBirNE-c"}},{"cell_type":"code","source":["alpha = 0.1\n","\n","env.reset()\n","tot_rewards = []\n","for _ in tqdm(range(repeat)):\n","    H = np.zeros(shape=(k))\n","    rewards = []\n","    for i in range(iter):\n","        action, probs = get_action(H)\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","        H[action] = H[action] + alpha * reward * (1 - probs[action])\n","        H[:action] = H[:action]  - alpha * reward * probs[:action]\n","        if action + 1 < k:\n","            H[action + 1:] = H[action + 1:]  - alpha * reward * probs[action + 1]\n","    env.reset()\n","    tot_rewards.append(rewards)\n","\n","o00 = average(tot_rewards)"],"metadata":{"id":"PmXFYl2rNIxG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the Gradient Bandit algorith with alpha = 0.1 and Baseline several times to average the results"],"metadata":{"id":"TzwKJHUcxf_q"}},{"cell_type":"code","source":["env.reset()\n","tot_rewards = []\n","for _ in tqdm(range(repeat)):\n","    H = np.zeros(shape=(k))\n","    R = 0\n","    rewards = []\n","    for i in range(iter):\n","        action, probs = get_action(H)\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","        H[action] = H[action] + alpha * (reward - R) * (1 - probs[action])\n","        H[:action] = H[:action]  - alpha * (reward - R) * probs[:action]\n","        if action + 1 < k:\n","            H[action + 1:] = H[action + 1:]  - alpha * (reward - R) * probs[action + 1]\n","        R = R + 1/(i + 1) * (reward - R)\n","    env.reset()\n","    tot_rewards.append(rewards)\n","\n","e00 = average(tot_rewards)"],"metadata":{"id":"gF4TXI_axhCY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we plot the results to compare the Gradient Bandit Algorithm with and without Baseline"],"metadata":{"id":"RCd4q2lDNTgp"}},{"cell_type":"code","source":["plt.plot(o0, color='blue', label='No Baseline, a = 0.4')\n","plt.plot(e01, color='olive', label='Baseline, a = 0.4')\n","plt.plot(o00, color='red', label='No Baseline, a = 0.1')\n","plt.plot(e00, color='green', label='Baseline, a = 0.1')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Average Reward\")\n","plt.title(\"Average Reward vs. Steps on 10 Armed Bandit\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"vYMT3L0GCPcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2tVbo3ZHQVxx"},"execution_count":null,"outputs":[]}]}