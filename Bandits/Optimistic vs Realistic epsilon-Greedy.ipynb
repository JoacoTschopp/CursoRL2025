{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPlQtp+34iBNr+4cd0BnmAv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Multiarmed Bandits: Optimistic Greedy versus Realistic epsilon-Greedy algorithms**\n","\n","In this notebook, we use OpenAI Gym to implement optimistic and realistic initializations"],"metadata":{"id":"76tfA-bMAk9i"}},{"cell_type":"code","source":["import gym"],"metadata":{"id":"l79GNCq0HjaP","executionInfo":{"status":"ok","timestamp":1715025240646,"user_tz":180,"elapsed":8,"user":{"displayName":"Cesar Caiafa","userId":"07160910981834058368"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Unfortunatley, Gym does not provide a bandit environment so we need to import it, lets install one with the command below:"],"metadata":{"id":"GGl2HVXbHYzn"}},{"cell_type":"code","source":["!git clone https://github.com/JKCooper2/gym-bandits.git > /dev/null 2>&1\n","!pip install /content/gym-bandits/. > /dev/null 2>&1"],"metadata":{"id":"LDeviPzjHfa4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4037fb2d-bd06-4b4c-e20d-52e999cb6b58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["Let's import needed packages"],"metadata":{"id":"CRyZ2QEeJB3k"}},{"cell_type":"code","source":["import gym_bandits\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from tqdm import tqdm"],"metadata":{"id":"1UXzA8QyH9nI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the epsilon-Greedy actions (Note: if epsilon=0 it corresponds to the Greedy action)"],"metadata":{"id":"U54G4uouJN7S"}},{"cell_type":"code","source":["def get_action(Q, e):\n","    if random.random() < e:\n","        return random.randint(0, 9)\n","    else:\n","        return np.argmax(Q)"],"metadata":{"id":"9-b0XhlrJVuq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We want to compare the Average Reward over repeated experiments for different values of epsilon.\n","\n","We define a function that compute the average of obtained reward over repeated experiments."],"metadata":{"id":"pGE4vvwzKQ3z"}},{"cell_type":"code","source":["def average(tot_rewards):\n","    avg = []\n","    for i in range(len(tot_rewards[0])):\n","        temp = 0\n","        for j in range(len(tot_rewards)):\n","            temp += tot_rewards[j][i]\n","        temp /= len(tot_rewards)\n","        avg.append(temp)\n","    return avg"],"metadata":{"id":"UFe6XPQWKxv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Main code: define an Environment for 10-armed Bandit whose rewards are normal distributed with standard deviation = 1 and a mean drawn from a normal distribution (0,1). Same as Sutton&Barto book"],"metadata":{"id":"gJXeronaLBgR"}},{"cell_type":"code","source":["k = 10  # Number of actions (arms)\n","iter = 1000 # Number of steps\n","repeat = 2000 # Number of repetitions\n","o0 = []  # Results for Optimistic Greedy algorithm\n","e01 = [] # Results for epsilon = 0.1\n","\n","np.random.seed(42)\n","env = gym.make(\"BanditTenArmedGaussian-v0\") # define Environment"],"metadata":{"id":"SGTPIh92LwOw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the Optimistic Greedy algorithm (epsilon = 0) several times to average the results"],"metadata":{"id":"dAEDMZfZL0Iq"}},{"cell_type":"code","source":["env.reset()\n","tot_rewards = []\n","e = 0\n","for _ in tqdm(range(repeat)):\n","    Q = 5 * np.ones(shape=(k))\n","    N = np.zeros(shape=(k))\n","    rewards = []\n","    for i in range(iter):\n","        action = get_action(Q, e)\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","        N[action] += 1\n","        Q[action] = Q[action] +  1/N[action] * (reward - Q[action])\n","    env.reset()\n","    tot_rewards.append(rewards)\n","\n","o0 = average(tot_rewards)"],"metadata":{"id":"UqkyUekIL9zY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the realistic epsilon-Greedy algorithm with epsilon = 0.01 several times to average the results"],"metadata":{"id":"laVkkOltM2MG"}},{"cell_type":"code","source":["env.reset()\n","tot_rewards = []\n","e = 0.1\n","for _ in tqdm(range(repeat)):\n","    Q = np.zeros(shape=(k))\n","    N = np.zeros(shape=(k))\n","    rewards = []\n","    for i in range(iter):\n","        action = get_action(Q, e)\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","        N[action] += 1\n","        Q[action] = Q[action] +  1/N[action] * (reward - Q[action])\n","    env.reset()\n","    tot_rewards.append(rewards)\n","\n","e01 = average(tot_rewards)"],"metadata":{"id":"Zlp6V7tPM-5E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we plot the results to compare the Greedy algorithm against the epsilon-Greedy algorithm"],"metadata":{"id":"RCd4q2lDNTgp"}},{"cell_type":"code","source":["plt.plot(o0, color='blue', label='Optimistic Init, e = 0')\n","plt.plot(e01, color='olive', label='0 init, e = 0.1')\n","plt.xlabel(\"Steps\")\n","plt.ylim(0, max(o0) + 0.2)\n","plt.ylabel(\"Average Reward\")\n","plt.title(\"Average Reward vs. Steps on 10 Armed Bandit\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"vYMT3L0GCPcF"},"execution_count":null,"outputs":[]}]}