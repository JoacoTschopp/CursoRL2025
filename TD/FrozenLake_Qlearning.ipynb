{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWHFsb7v4H0bmxl4jmSO3C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Frozen Lake**\n","\n","In this notebook, we implement Q-learning on OpenAI Gym to the Frozen Lake environment (Lapan book, chapter 6)"],"metadata":{"id":"76tfA-bMAk9i"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Kt1EVEaTRVhA"}},{"cell_type":"code","source":["#!/usr/bin/env python3\n","import gym\n","import collections\n","!pip install tensorboardX\n","from tensorboardX import SummaryWriter\n","\n","ENV_NAME = \"FrozenLake-v1\"\n","GAMMA = 0.9\n","ALPHA = 0.2\n","TEST_EPISODES = 20"],"metadata":{"id":"l79GNCq0HjaP","executionInfo":{"status":"ok","timestamp":1717357530116,"user_tz":180,"elapsed":19789,"user":{"displayName":"Cesar Caiafa","userId":"07160910981834058368"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa346211-7820-460c-cfd6-63493e807f85"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}]},{"cell_type":"markdown","source":["Define Agent Class"],"metadata":{"id":"GGl2HVXbHYzn"}},{"cell_type":"code","source":["class Agent:\n","    def __init__(self):\n","      # Intitialize Agent\n","      self.env = gym.make(ENV_NAME)\n","      self.state = self.env.reset()\n","      self.values = collections.defaultdict(float)\n","\n","    def sample_env(self):\n","      # We sample a random action from the action space and return the tupple\n","      # [old_state, action, reward, new_state]\n","      action = self.env.action_space.sample()\n","      old_state = self.state\n","      new_state, reward, is_done, _ = self.env.step(action)\n","      self.state = self.env.reset() if is_done else new_state\n","      return old_state, action, reward, new_state\n","\n","    def best_value_and_action(self, state):\n","      # Given a state, we search for the best action (having the best value)\n","      best_value, best_action = None, None\n","      for action in range(self.env.action_space.n):\n","          action_value = self.values[(state, action)]\n","          if best_value is None or best_value < action_value:\n","              best_value = action_value\n","              best_action = action\n","      return best_value, best_action\n","\n","    def value_update(self, s, a, r, next_s):\n","      # Q-Learning update rule\n","      best_v, _ = self.best_value_and_action(next_s)\n","      new_v = r + GAMMA * best_v\n","      old_v = self.values[(s, a)]\n","      self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n","\n","    def play_episode(self, env):\n","      # Run one episode to test a policy\n","      total_reward = 0.0\n","      state = env.reset()\n","      while True:\n","          _, action = self.best_value_and_action(state)\n","          new_state, reward, is_done, _= env.step(action)\n","          total_reward += reward\n","          if is_done:\n","              break\n","          state = new_state\n","      return total_reward"],"metadata":{"id":"LDeviPzjHfa4","executionInfo":{"status":"ok","timestamp":1717357537080,"user_tz":180,"elapsed":3,"user":{"displayName":"Cesar Caiafa","userId":"07160910981834058368"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Run Q-Learning"],"metadata":{"id":"CRyZ2QEeJB3k"}},{"cell_type":"code","source":["test_env = gym.make(ENV_NAME)\n","agent = Agent()\n","writer = SummaryWriter(comment=\"-q-learning\")\n","\n","iter_no = 0\n","best_reward = 0.0\n","while True:\n","    iter_no += 1\n","    s, a, r, next_s = agent.sample_env()\n","    agent.value_update(s, a, r, next_s)\n","\n","    reward = 0.0\n","    for _ in range(TEST_EPISODES):\n","        reward += agent.play_episode(test_env)\n","    reward /= TEST_EPISODES\n","    writer.add_scalar(\"reward\", reward, iter_no)\n","    if reward > best_reward:\n","        print(\"Best reward updated %.3f -> %.3f\" % (\n","            best_reward, reward))\n","        best_reward = reward\n","    if reward > 0.95:\n","        print(\"Solved in %d iterations!\" % iter_no)\n","        break\n","writer.close()\n"],"metadata":{"id":"1UXzA8QyH9nI","executionInfo":{"status":"ok","timestamp":1717359672859,"user_tz":180,"elapsed":288301,"user":{"displayName":"Cesar Caiafa","userId":"07160910981834058368"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4879105f-23fc-4ee2-ea42-090758182234"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best reward updated 0.000 -> 0.050\n","Best reward updated 0.050 -> 0.100\n","Best reward updated 0.100 -> 0.150\n","Best reward updated 0.150 -> 0.200\n","Best reward updated 0.200 -> 0.300\n","Best reward updated 0.300 -> 0.350\n","Best reward updated 0.350 -> 0.450\n","Best reward updated 0.450 -> 0.500\n","Best reward updated 0.500 -> 0.600\n","Best reward updated 0.600 -> 0.650\n","Best reward updated 0.650 -> 0.750\n","Best reward updated 0.750 -> 0.800\n","Best reward updated 0.800 -> 0.850\n","Best reward updated 0.850 -> 0.900\n","Best reward updated 0.900 -> 0.950\n","Best reward updated 0.950 -> 1.000\n","Solved in 19176 iterations!\n"]}]}]}